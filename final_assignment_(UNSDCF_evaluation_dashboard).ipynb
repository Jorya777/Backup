{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jorya777/Backup/blob/main/final_assignment_(UNSDCF_evaluation_dashboard).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "OyWcaPWe00eg",
        "outputId": "0c2eea26-f095-4b40-fe27-3786cb2467ef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: streamlit in /usr/local/lib/python3.12/dist-packages (1.50.0)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.12/dist-packages (5.24.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: altair!=5.4.0,!=5.4.1,<6,>=4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<7,>=4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (5.5.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (8.3.0)\n",
            "Requirement already satisfied: packaging<26,>=20 in /usr/local/lib/python3.12/dist-packages (from streamlit) (25.0)\n",
            "Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (11.3.0)\n",
            "Requirement already satisfied: protobuf<7,>=3.20 in /usr/local/lib/python3.12/dist-packages (from streamlit) (5.29.5)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (18.1.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.12/dist-packages (from streamlit) (2.32.4)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (8.5.0)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.12/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (4.15.0)\n",
            "Requirement already satisfied: watchdog<7,>=2.1.5 in /usr/local/lib/python3.12/dist-packages (from streamlit) (6.0.0)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.12/dist-packages (from streamlit) (3.1.45)\n",
            "Requirement already satisfied: pydeck<1,>=0.8.0b4 in /usr/local/lib/python3.12/dist-packages (from streamlit) (0.9.1)\n",
            "Requirement already satisfied: tornado!=6.5.0,<7,>=6.0.3 in /usr/local/lib/python3.12/dist-packages (from streamlit) (6.5.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (3.1.6)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (4.25.1)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (2.9.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (2025.10.5)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (3.0.3)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (25.4.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (2025.9.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (0.37.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (0.27.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install streamlit plotly pandas numpy\n",
        "!wget -q https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64.deb\n",
        "!dpkg -i cloudflared-linux-amd64.deb >/dev/null 2>&1 || true\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NhKH_goL1uBP",
        "outputId": "b454c5a7-3d3b-47b9-a16c-ce0a7960f4d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QFYxYXXDh_mQ",
        "outputId": "5d14138a-6ab0-4a7f-e9c4-55e947793c93"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Collecting python-docx\n",
            "  Downloading python_docx-1.2.0-py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from python-docx) (5.4.0)\n",
            "Requirement already satisfied: typing_extensions>=4.9.0 in /usr/local/lib/python3.12/dist-packages (from python-docx) (4.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2024.11.6)\n",
            "Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_docx-1.2.0-py3-none-any.whl (252 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.0/253.0 kB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: python-docx, PyPDF2\n",
            "Successfully installed PyPDF2-3.0.1 python-docx-1.2.0\n"
          ]
        }
      ],
      "source": [
        "!pip install PyPDF2 python-docx tqdm nltk\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ubyyYHCeisyA",
        "outputId": "d150294d-b6ac-4746-fc8b-7ef148bc7c97"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('vader_lexicon')\n",
        "nltk.download('punkt_tab')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================================\n",
        "# UNSDCF Evaluation Reports Text Extraction and Analysis\n",
        "# Generates:\n",
        "#   1. relevant_sentences_UNSDCF_filtered.csv\n",
        "#   2. word_frequency_UNSDCF.csv\n",
        "#   3. actor_cooccurrence_UNSDCF.csv\n",
        "# ==========================================================\n",
        "\n",
        "import os\n",
        "import re\n",
        "import pandas as pd\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from PyPDF2 import PdfReader\n",
        "from docx import Document\n",
        "from tqdm import tqdm\n",
        "import nltk\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from itertools import combinations\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# Init\n",
        "# ----------------------------------------------------------\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"vader_lexicon\")\n",
        "nltk.download(\"stopwords\")\n",
        "\n",
        "DATA_DIR = \"/content/drive/MyDrive/evaluation_reports\"\n",
        "OUTPUT_SENT = \"/content/drive/MyDrive/relevant_sentences_UNSDCF_filtered.csv\"\n",
        "OUTPUT_WORD = \"/content/drive/MyDrive/word_frequency_UNSDCF.csv\"\n",
        "OUTPUT_COOC = \"/content/drive/MyDrive/actor_cooccurrence_UNSDCF.csv\"\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# Step 1: Extract text\n",
        "# ----------------------------------------------------------\n",
        "def extract_text(file_path):\n",
        "    text = \"\"\n",
        "    try:\n",
        "        if file_path.endswith(\".pdf\"):\n",
        "            reader = PdfReader(file_path)\n",
        "            for page in reader.pages:\n",
        "                t = page.extract_text()\n",
        "                if t:\n",
        "                    text += t + \"\\n\"\n",
        "        elif file_path.endswith(\".docx\"):\n",
        "            doc = Document(file_path)\n",
        "            text = \"\\n\".join([p.text for p in doc.paragraphs if p.text.strip()])\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ Failed to read {file_path}: {e}\")\n",
        "    return text\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# Step 2: Extract relevant sentences\n",
        "# ----------------------------------------------------------\n",
        "KEYWORDS = [\"DCO\", \"RC\", \"UNCT\"]\n",
        "\n",
        "def extract_relevant_sentences(text, country, filename):\n",
        "    sentences = sent_tokenize(text)\n",
        "    data = []\n",
        "    for i, s in enumerate(sentences):\n",
        "        if any(k in s for k in KEYWORDS):\n",
        "            context = \" \".join(sentences[max(0, i-1):min(len(sentences), i+2)])\n",
        "            data.append({\n",
        "                \"Country\": country,\n",
        "                \"Sentence\": context.strip(),\n",
        "                \"SourceFile\": filename\n",
        "            })\n",
        "    return pd.DataFrame(data)\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# Step 3: Loop through reports\n",
        "# ----------------------------------------------------------\n",
        "all_data = []\n",
        "for file in tqdm(os.listdir(DATA_DIR)):\n",
        "    if not (file.endswith(\".pdf\") or file.endswith(\".docx\")):\n",
        "        continue\n",
        "    file_path = os.path.join(DATA_DIR, file)\n",
        "    country = re.sub(r\"[^A-Za-z]\", \" \", os.path.splitext(file)[0]).split()[0]\n",
        "    text = extract_text(file_path)\n",
        "    df = extract_relevant_sentences(text, country, file)\n",
        "    all_data.append(df)\n",
        "\n",
        "df_all = pd.concat(all_data, ignore_index=True)\n",
        "print(f\"✅ Extracted {len(df_all)} relevant sentences.\")\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# Step 4: Sentiment analysis + Actor label\n",
        "# ----------------------------------------------------------\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "df_all[\"Sentiment\"] = df_all[\"Sentence\"].apply(lambda x: sia.polarity_scores(x)[\"compound\"])\n",
        "df_all[\"Sentiment_Label\"] = df_all[\"Sentiment\"].apply(\n",
        "    lambda s: \"Positive\" if s > 0.2 else (\"Negative\" if s < -0.2 else \"Neutral\")\n",
        ")\n",
        "\n",
        "def detect_actor(sentence):\n",
        "    actors = []\n",
        "    for a in [\"DCO\", \"RC\", \"UNCT\"]:\n",
        "        if re.search(rf\"\\b{a}\\b\", sentence):\n",
        "            actors.append(a)\n",
        "    return \", \".join(actors) if actors else \"Unspecified\"\n",
        "\n",
        "df_all[\"Actor\"] = df_all[\"Sentence\"].apply(detect_actor)\n",
        "df_all = df_all[df_all[\"Actor\"] != \"Unspecified\"]\n",
        "df_all.to_csv(OUTPUT_SENT, index=False)\n",
        "print(f\"📂 Saved filtered sentences to: {OUTPUT_SENT}\")\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# Step 5: Word frequency\n",
        "# ----------------------------------------------------------\n",
        "def clean_text(s):\n",
        "    if not isinstance(s, str):\n",
        "        return \"\"\n",
        "    s = s.lower()\n",
        "    s = re.sub(r\"[^a-z\\s]\", \" \", s)\n",
        "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
        "    return s\n",
        "\n",
        "df_all[\"Sentence_clean\"] = df_all[\"Sentence\"].astype(str).apply(clean_text)\n",
        "\n",
        "stop_list = list(set(stopwords.words(\"english\")))\n",
        "vectorizer = CountVectorizer(stop_words=stop_list, max_features=1000, min_df=2)\n",
        "X = vectorizer.fit_transform(df_all[\"Sentence_clean\"])\n",
        "\n",
        "word_freq = pd.DataFrame({\n",
        "    \"word\": vectorizer.get_feature_names_out(),\n",
        "    \"count\": X.toarray().sum(axis=0)\n",
        "}).sort_values(by=\"count\", ascending=False)\n",
        "word_freq.to_csv(OUTPUT_WORD, index=False)\n",
        "print(f\"✅ Saved word frequency to: {OUTPUT_WORD}\")\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# Step 6: Actor co-occurrence\n",
        "# ----------------------------------------------------------\n",
        "cooccurrence = []\n",
        "for _, row in df_all.iterrows():\n",
        "    found = [a for a in [\"UNCT\", \"RC\", \"DCO\"] if a in row[\"Sentence\"]]\n",
        "    if len(found) > 1:\n",
        "        for combo in combinations(found, 2):\n",
        "            cooccurrence.append(combo)\n",
        "\n",
        "if cooccurrence:\n",
        "    df_cooc = pd.DataFrame(cooccurrence, columns=[\"Actor1\", \"Actor2\"])\n",
        "    df_cooc[\"pair\"] = df_cooc.apply(lambda x: \" & \".join(sorted([x[\"Actor1\"], x[\"Actor2\"]])), axis=1)\n",
        "    df_cooc = df_cooc.groupby(\"pair\").size().reset_index(name=\"count\").sort_values(by=\"count\", ascending=False)\n",
        "    df_cooc.to_csv(OUTPUT_COOC, index=False)\n",
        "    print(f\"✅ Saved actor co-occurrence to: {OUTPUT_COOC}\")\n",
        "else:\n",
        "    print(\"⚠️ No co-occurring actors found.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pqUz2KIzzbe3",
        "outputId": "82927a98-6e73-4050-b282-038e9e186bf2"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "100%|██████████| 6/6 [00:46<00:00,  7.82s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Extracted 737 relevant sentences.\n",
            "📂 Saved filtered sentences to: /content/drive/MyDrive/relevant_sentences_UNSDCF_filtered.csv\n",
            "✅ Saved word frequency to: /content/drive/MyDrive/word_frequency_UNSDCF.csv\n",
            "✅ Saved actor co-occurrence to: /content/drive/MyDrive/actor_cooccurrence_UNSDCF.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vWJH8x-km_Q9",
        "outputId": "34d4eeb7-e874-439f-f48f-67a2f08defdb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /content/drive/MyDrive/app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile /content/drive/MyDrive/app.py\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "\n",
        "st.set_page_config(page_title=\"UNSDCF Evaluation Dashboard\", layout=\"wide\")\n",
        "\n",
        "st.title(\"🌍 United Nations Sustainable Development Cooperation Framework Evaluation Dashboard\")\n",
        "\n",
        "\n",
        "file_path = \"/content/drive/MyDrive/2021-2023 evaluation expenditures analysis .xlsx\"\n",
        "df_spend = pd.read_excel(file_path)\n",
        "df_spend.columns = df_spend.columns.str.strip()\n",
        "df_spend.rename(columns={\n",
        "    \"Evaluation expenditure($)\": \"Evaluation Spending ($)\",\n",
        "    \"Program Expenditure\": \"Program Expenditure\",\n",
        "    \"The proportion of Evaluation Expenditure to Program Expenditure\": \"Eval Ratio (%)\"\n",
        "}, inplace=True)\n",
        "for c in [\"Evaluation Spending ($)\", \"Program Expenditure\", \"Eval Ratio (%)\"]:\n",
        "    df_spend[c] = pd.to_numeric(df_spend[c], errors=\"coerce\")\n",
        "df_spend.dropna(subset=[\"Eval Ratio (%)\"], inplace=True)\n",
        "\n",
        "\n",
        "st.subheader(\"🌍 Evaluation Countries (2021–2023)\")\n",
        "fig_map = px.scatter_geo(df_spend, locations=\"Country\", locationmode=\"country names\",\n",
        "                         hover_name=\"Country\", hover_data={\"Evaluation year \": True},\n",
        "                         text=\"Evaluation year \", projection=\"natural earth\")\n",
        "st.plotly_chart(fig_map, use_container_width=True)\n",
        "\n",
        "\n",
        "st.subheader(\"💰 Evaluation vs Programme Expenditure\")\n",
        "fig_scatter = px.scatter(df_spend, x=\"Program Expenditure\", y=\"Eval Ratio (%)\",\n",
        "                         size=\"Evaluation Spending ($)\",\n",
        "                         color=\"Region\" if \"Region\" in df_spend.columns else \"Country\",\n",
        "                         hover_name=\"Country\")\n",
        "st.plotly_chart(fig_scatter, use_container_width=True)\n",
        "\n",
        "\n",
        "CRITERIA = ['relevance','coherence','effectiveness','efficiency','orientation towards impact','sustainability']\n",
        "countries_eval = [\"Azerbaijan\",\"Uganda\",\"Serbia\",\"Indonesia\",\"Panama\",\"Bosnia and Herzegovina\"]\n",
        "scores = {\n",
        "    \"Azerbaijan\":[4,3,4,3,3,3],\n",
        "    \"Uganda\":[4,2,4,3,3,3],\n",
        "    \"Serbia\":[4,2,4,3,3,3],\n",
        "    \"Indonesia\":[5,3,4,3,4,3],\n",
        "    \"Panama\":[4,3,3,3,3,2],\n",
        "    \"Bosnia and Herzegovina\":[4,2,4,3,3,3]\n",
        "}\n",
        "df_scores = pd.DataFrame([{\"Country\":c,\"Criterion\":crit,\"Score\":scores[c][i]}\n",
        "                          for c in countries_eval for i,crit in enumerate(CRITERIA)])\n",
        "country = st.sidebar.selectbox(\"Select Country\", countries_eval)\n",
        "fig_radar = px.line_polar(df_scores[df_scores[\"Country\"]==country],\n",
        "                          r=\"Score\", theta=\"Criterion\", line_close=True)\n",
        "st.plotly_chart(fig_radar, use_container_width=True)\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# Text analysis\n",
        "# ----------------------------------------------------------\n",
        "st.header(\" Text Analysis: Mentions of DCO / RC / UNCT\")\n",
        "\n",
        "try:\n",
        "    df_mentions = pd.read_csv(\"/content/drive/MyDrive/relevant_sentences_UNSDCF_scored.csv\")\n",
        "    df_words = pd.read_csv(\"/content/drive/MyDrive/word_frequency_UNSDCF.csv\")\n",
        "\n",
        "    st.subheader(\"📑 Sample Extracted Mentions\")\n",
        "    st.dataframe(df_mentions.head(10))\n",
        "\n",
        "    st.subheader(\"📊 Sentiment Distribution\")\n",
        "    sent_summary = df_mentions.groupby(\"Sentiment_Label\").size().reset_index(name=\"Count\")\n",
        "    fig_sent = px.bar(sent_summary, x=\"Sentiment_Label\", y=\"Count\", color=\"Sentiment_Label\")\n",
        "    st.plotly_chart(fig_sent, use_container_width=True)\n",
        "\n",
        "    st.subheader(\"🔤 Top Keywords in Mentions\")\n",
        "    top_words = df_words.head(20)\n",
        "    fig_words = px.bar(top_words, x=\"word\", y=\"count\", title=\"Top 20 Words in DCO/RC/UNCT Mentions\", color=\"count\")\n",
        "    st.plotly_chart(fig_words, use_container_width=True)\n",
        "\n",
        "except Exception as e:\n",
        "    st.warning(f\"⚠️ Text analysis results not found or failed to load: {e}\")\n",
        "\n",
        "st.markdown(\"---\")\n",
        "st.markdown(\"© United Nations DCO – Data visualization for learning purposes\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cuHi0VMv1atS",
        "outputId": "637e4f67-34ce-4e39-9989-aed086b69aae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Cloudflare Tunnel … (keep this cell running)\n",
            "2025-10-25T19:31:52Z INF Thank you for trying Cloudflare Tunnel. Doing so, without a Cloudflare account, is a quick way to experiment and try it out. However, be aware that these account-less Tunnels have no uptime guarantee, are subject to the Cloudflare Online Services Terms of Use (https://www.cloudflare.com/website-terms/), and Cloudflare reserves the right to investigate your use of Tunnels for violations of such terms. If you intend to use Tunnels in production you should use a pre-created named tunnel by following: https://developers.cloudflare.com/cloudflare-one/connections/connect-apps\n",
            "2025-10-25T19:31:52Z INF Requesting new quick Tunnel on trycloudflare.com...\n",
            "2025-10-25T19:31:55Z INF +--------------------------------------------------------------------------------------------+\n",
            "2025-10-25T19:31:55Z INF |  Your quick Tunnel has been created! Visit it at (it may take some time to be reachable):  |\n",
            "2025-10-25T19:31:55Z INF |  https://priest-afterwards-red-concerned.trycloudflare.com                                 |\n",
            "\n",
            "🚀 Streamlit app URL: https://priest-afterwards-red-concerned.trycloudflare.com\n",
            "No password required. Keep this cell running while you use the app.\n"
          ]
        }
      ],
      "source": [
        "import subprocess, time, re, sys\n",
        "\n",
        "PORT = \"8501\"\n",
        "# 启动 Streamlit 后台服务\n",
        "streamlit = subprocess.Popen(\n",
        "    [\"streamlit\", \"run\", \"app.py\", \"--server.port\", PORT, \"--server.headless\", \"true\"],\n",
        "    stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True\n",
        ")\n",
        "time.sleep(2)\n",
        "\n",
        "# 启动 Cloudflare Tunnel\n",
        "tunnel = subprocess.Popen(\n",
        "    [\"cloudflared\", \"tunnel\", \"--url\", f\"http://localhost:{PORT}\", \"--no-autoupdate\"],\n",
        "    stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True\n",
        ")\n",
        "\n",
        "print(\"Starting Cloudflare Tunnel … (keep this cell running)\")\n",
        "for line in tunnel.stdout:\n",
        "    sys.stdout.write(line)\n",
        "    sys.stdout.flush()\n",
        "    m = re.search(r\"https://[a-z0-9-]+\\.trycloudflare\\.com\", line)\n",
        "    if m:\n",
        "        print(\"\\n🚀 Streamlit app URL:\", m.group(0))\n",
        "        print(\"No password required. Keep this cell running while you use the app.\")\n",
        "        break\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM/y8bYydr6DnIdS0yisvyb",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}